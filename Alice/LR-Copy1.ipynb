{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = ['time%s' % i for i in range(1, 11)]\n",
    "\n",
    "train_df = pd.read_csv('train_sessions.csv',\n",
    "                       index_col='session_id', parse_dates=times)\n",
    "test_df = pd.read_csv('test_sessions.csv',\n",
    "                      index_col='session_id', parse_dates=times)\n",
    "\n",
    "# Sort the data by time\n",
    "train_df = train_df.sort_values(by='time1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Websites total: 48371\n"
     ]
    }
   ],
   "source": [
    "sites = ['site%s' % i for i in range(1, 11)]\n",
    "train_df[sites] = train_df[sites].fillna(0).astype('int')\n",
    "test_df[sites] = test_df[sites].fillna(0).astype('int')\n",
    "\n",
    "# Load websites dictionary\n",
    "with open(r\"site_dic.pkl\", \"rb\") as input_file:\n",
    "    site_dict = pickle.load(input_file)\n",
    "\n",
    "# Create dataframe for the dictionary\n",
    "sites_dict = pd.DataFrame(list(site_dict.keys()), index=list(site_dict.values()), columns=['site'])\n",
    "print(u'Websites total:', sites_dict.shape[0])\n",
    "#sites_dict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small\n",
    "train_df[sites].fillna(0).to_csv('train_sessions_text.txt', \n",
    "                                 sep=' ', index=None, header=None)\n",
    "test_df[sites].fillna(0).to_csv('test_sessions_text.txt', \n",
    "                                sep=' ', index=None, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our target variable\n",
    "y = train_df['target'].values\n",
    "\n",
    "# United dataframe of the initial data \n",
    "full_df = pd.concat([train_df.drop('target', axis=1), test_df])\n",
    "\n",
    "# Index to split the training and test data sets\n",
    "train_df = train_df.drop('target', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_df, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site1</th>\n",
       "      <th>time1</th>\n",
       "      <th>site2</th>\n",
       "      <th>time2</th>\n",
       "      <th>site3</th>\n",
       "      <th>time3</th>\n",
       "      <th>site4</th>\n",
       "      <th>time4</th>\n",
       "      <th>site5</th>\n",
       "      <th>time5</th>\n",
       "      <th>site6</th>\n",
       "      <th>time6</th>\n",
       "      <th>site7</th>\n",
       "      <th>time7</th>\n",
       "      <th>site8</th>\n",
       "      <th>time8</th>\n",
       "      <th>site9</th>\n",
       "      <th>time9</th>\n",
       "      <th>site10</th>\n",
       "      <th>time10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>session_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>85699</th>\n",
       "      <td>8196</td>\n",
       "      <td>2014-03-17 14:53:12</td>\n",
       "      <td>8196</td>\n",
       "      <td>2014-03-17 14:53:13</td>\n",
       "      <td>33</td>\n",
       "      <td>2014-03-17 14:53:14</td>\n",
       "      <td>8196</td>\n",
       "      <td>2014-03-17 14:53:14</td>\n",
       "      <td>35</td>\n",
       "      <td>2014-03-17 14:53:14</td>\n",
       "      <td>29</td>\n",
       "      <td>2014-03-17 14:53:14</td>\n",
       "      <td>29</td>\n",
       "      <td>2014-03-17 14:53:15</td>\n",
       "      <td>8196</td>\n",
       "      <td>2014-03-17 14:53:15</td>\n",
       "      <td>8196</td>\n",
       "      <td>2014-03-17 14:53:16</td>\n",
       "      <td>8196</td>\n",
       "      <td>2014-03-17 14:53:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69489</th>\n",
       "      <td>35</td>\n",
       "      <td>2014-01-29 16:30:35</td>\n",
       "      <td>2882</td>\n",
       "      <td>2014-01-29 16:30:35</td>\n",
       "      <td>2877</td>\n",
       "      <td>2014-01-29 16:30:37</td>\n",
       "      <td>2880</td>\n",
       "      <td>2014-01-29 16:30:37</td>\n",
       "      <td>35</td>\n",
       "      <td>2014-01-29 16:30:38</td>\n",
       "      <td>22</td>\n",
       "      <td>2014-01-29 16:30:38</td>\n",
       "      <td>33</td>\n",
       "      <td>2014-01-29 16:30:38</td>\n",
       "      <td>23</td>\n",
       "      <td>2014-01-29 16:30:38</td>\n",
       "      <td>2880</td>\n",
       "      <td>2014-01-29 16:30:38</td>\n",
       "      <td>173</td>\n",
       "      <td>2014-01-29 16:30:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161596</th>\n",
       "      <td>38</td>\n",
       "      <td>2013-11-25 10:35:57</td>\n",
       "      <td>21677</td>\n",
       "      <td>2013-11-25 10:35:57</td>\n",
       "      <td>36</td>\n",
       "      <td>2013-11-25 10:35:57</td>\n",
       "      <td>30</td>\n",
       "      <td>2013-11-25 10:35:57</td>\n",
       "      <td>29</td>\n",
       "      <td>2013-11-25 10:35:57</td>\n",
       "      <td>31</td>\n",
       "      <td>2013-11-25 10:35:57</td>\n",
       "      <td>21677</td>\n",
       "      <td>2013-11-25 10:35:58</td>\n",
       "      <td>21677</td>\n",
       "      <td>2013-11-25 10:36:00</td>\n",
       "      <td>39</td>\n",
       "      <td>2013-11-25 10:36:04</td>\n",
       "      <td>39</td>\n",
       "      <td>2013-11-25 10:36:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23054</th>\n",
       "      <td>881</td>\n",
       "      <td>2014-03-17 10:06:25</td>\n",
       "      <td>78</td>\n",
       "      <td>2014-03-17 10:06:25</td>\n",
       "      <td>80</td>\n",
       "      <td>2014-03-17 10:06:30</td>\n",
       "      <td>881</td>\n",
       "      <td>2014-03-17 10:06:31</td>\n",
       "      <td>881</td>\n",
       "      <td>2014-03-17 10:06:40</td>\n",
       "      <td>80</td>\n",
       "      <td>2014-03-17 10:06:43</td>\n",
       "      <td>881</td>\n",
       "      <td>2014-03-17 10:06:46</td>\n",
       "      <td>881</td>\n",
       "      <td>2014-03-17 10:06:55</td>\n",
       "      <td>80</td>\n",
       "      <td>2014-03-17 10:07:00</td>\n",
       "      <td>881</td>\n",
       "      <td>2014-03-17 10:07:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53317</th>\n",
       "      <td>35</td>\n",
       "      <td>2014-02-18 14:40:40</td>\n",
       "      <td>29</td>\n",
       "      <td>2014-02-18 14:40:40</td>\n",
       "      <td>4368</td>\n",
       "      <td>2014-02-18 14:40:40</td>\n",
       "      <td>33</td>\n",
       "      <td>2014-02-18 14:40:40</td>\n",
       "      <td>37</td>\n",
       "      <td>2014-02-18 14:40:49</td>\n",
       "      <td>21</td>\n",
       "      <td>2014-02-18 14:41:03</td>\n",
       "      <td>39</td>\n",
       "      <td>2014-02-18 14:41:13</td>\n",
       "      <td>38</td>\n",
       "      <td>2014-02-18 14:41:13</td>\n",
       "      <td>17744</td>\n",
       "      <td>2014-02-18 14:41:26</td>\n",
       "      <td>17743</td>\n",
       "      <td>2014-02-18 14:41:26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            site1               time1  site2               time2  site3  \\\n",
       "session_id                                                                \n",
       "85699        8196 2014-03-17 14:53:12   8196 2014-03-17 14:53:13     33   \n",
       "69489          35 2014-01-29 16:30:35   2882 2014-01-29 16:30:35   2877   \n",
       "161596         38 2013-11-25 10:35:57  21677 2013-11-25 10:35:57     36   \n",
       "23054         881 2014-03-17 10:06:25     78 2014-03-17 10:06:25     80   \n",
       "53317          35 2014-02-18 14:40:40     29 2014-02-18 14:40:40   4368   \n",
       "\n",
       "                         time3  site4               time4  site5  \\\n",
       "session_id                                                         \n",
       "85699      2014-03-17 14:53:14   8196 2014-03-17 14:53:14     35   \n",
       "69489      2014-01-29 16:30:37   2880 2014-01-29 16:30:37     35   \n",
       "161596     2013-11-25 10:35:57     30 2013-11-25 10:35:57     29   \n",
       "23054      2014-03-17 10:06:30    881 2014-03-17 10:06:31    881   \n",
       "53317      2014-02-18 14:40:40     33 2014-02-18 14:40:40     37   \n",
       "\n",
       "                         time5  site6               time6  site7  \\\n",
       "session_id                                                         \n",
       "85699      2014-03-17 14:53:14     29 2014-03-17 14:53:14     29   \n",
       "69489      2014-01-29 16:30:38     22 2014-01-29 16:30:38     33   \n",
       "161596     2013-11-25 10:35:57     31 2013-11-25 10:35:57  21677   \n",
       "23054      2014-03-17 10:06:40     80 2014-03-17 10:06:43    881   \n",
       "53317      2014-02-18 14:40:49     21 2014-02-18 14:41:03     39   \n",
       "\n",
       "                         time7  site8               time8  site9  \\\n",
       "session_id                                                         \n",
       "85699      2014-03-17 14:53:15   8196 2014-03-17 14:53:15   8196   \n",
       "69489      2014-01-29 16:30:38     23 2014-01-29 16:30:38   2880   \n",
       "161596     2013-11-25 10:35:58  21677 2013-11-25 10:36:00     39   \n",
       "23054      2014-03-17 10:06:46    881 2014-03-17 10:06:55     80   \n",
       "53317      2014-02-18 14:41:13     38 2014-02-18 14:41:13  17744   \n",
       "\n",
       "                         time9  site10              time10  \n",
       "session_id                                                  \n",
       "85699      2014-03-17 14:53:16    8196 2014-03-17 14:53:17  \n",
       "69489      2014-01-29 16:30:38     173 2014-01-29 16:30:38  \n",
       "161596     2013-11-25 10:36:04      39 2013-11-25 10:36:05  \n",
       "23054      2014-03-17 10:07:00     881 2014-03-17 10:07:02  \n",
       "53317      2014-02-18 14:41:26   17743 2014-02-18 14:41:26  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(253561, 50000) (82797, 50000)\n",
      "CPU times: user 9.5 s, sys: 185 ms, total: 9.69 s\n",
      "Wall time: 7.43 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cv = CountVectorizer(ngram_range=(1, 3), max_features=50000)\n",
    "with open('train_sessions_text.txt') as inp_train_file:\n",
    "    X_train = cv.fit_transform(inp_train_file)\n",
    "with open('test_sessions_text.txt') as inp_test_file:\n",
    "    X_test = cv.transform(inp_test_file)\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_auc_lr_valid(X, y, C=1.0, seed=17, ratio = 0.9):\n",
    "    # Split the data into the training and validation sets\n",
    "    idx = int(round(X.shape[0] * ratio))\n",
    "    # Classifier training\n",
    "    lr = LogisticRegression(C=C, random_state=seed, solver='lbfgs', max_iter=2000).fit(X[:idx, :], y[:idx])\n",
    "    # Prediction for validation set\n",
    "    y_pred = lr.predict_proba(X[idx:, :])[:, 1]\n",
    "    # Calculate the quality\n",
    "    score = roc_auc_score(y[idx:], y_pred)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9132519600597074\n",
      "CPU times: user 1min 28s, sys: 1.49 s, total: 1min 30s\n",
      "Wall time: 7.61 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Calculate metric on the validation set\n",
    "print(get_auc_lr_valid(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_newfeatures = pd.DataFrame(index=train_df.index)\n",
    "test_df_newfeatures = pd.DataFrame(index=test_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_newfeatures['year'] = train_df['time1'].apply(lambda ts : ts.year * 100 + ts.month)\n",
    "test_df_newfeatures['year'] = test_df['time1'].apply(lambda ts : ts.year * 100 + ts.month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_newfeatures['hour'] = train_df['time1'].apply(lambda ts : ts.hour)\n",
    "test_df_newfeatures['hour'] = test_df['time1'].apply(lambda ts : ts.hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is time between 5 and 13\n",
    "train_df_newfeatures['morning'] = ((train_df_newfeatures['hour'] > 5) & (train_df_newfeatures['hour'] < 13)) * 1 - 0.5\n",
    "test_df_newfeatures['morning'] = ((test_df_newfeatures['hour'] > 5) & (train_df_newfeatures['hour'] < 13)) * 1- 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = ['time%s' % i for i in range(1, 11)]\n",
    "train_df_newfeatures['session_lenght'] = (train_df[times].max(axis = 1) - train_df[times].min(axis = 1)).apply(lambda ts: ts.seconds)\n",
    "test_df_newfeatures['session_lenght'] = (test_df[times].max(axis = 1) - test_df[times].min(axis = 1)).apply(lambda ts: ts.seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.fit(test_df_newfeatures['year'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_newfeatures['year_scaled'] = scaler.fit_transform(train_df_newfeatures['year'].values.reshape(-1,1))\n",
    "test_df_newfeatures['year_scaled'] = scaler.transform(test_df_newfeatures['year'].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.fit(test_df_newfeatures['hour'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_newfeatures['hour_scaled'] = scaler.fit_transform(train_df_newfeatures['hour'].values.reshape(-1,1))\n",
    "test_df_newfeatures['hour_scaled'] = scaler.transform(test_df_newfeatures['hour'].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale session Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.fit(test_df_newfeatures['session_lenght'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_newfeatures['session_lenght_scaled'] = scaler.fit_transform(train_df_newfeatures['session_lenght'].values.reshape(-1,1))\n",
    "test_df_newfeatures['session_lenght_scaled'] = scaler.transform(test_df_newfeatures['session_lenght'].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add new features to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_new = csr_matrix(hstack([X_train, train_df_newfeatures[['year_scaled', 'hour_scaled', 'morning', 'session_lenght_scaled']]]))\n",
    "X_test_new = csr_matrix(hstack([X_test, test_df_newfeatures[['year_scaled', 'hour_scaled', 'morning', 'session_lenght_scaled']]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((253561, 50000), (253561, 50004))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_train_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((82797, 50000), (82797, 50004))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, X_test_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9547832159471328"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_auc_lr_valid(X_train_new, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_submission_file(predicted_labels, out_file,\n",
    "                             target='target', index_label=\"session_id\"):\n",
    "    predicted_df = pd.DataFrame(predicted_labels,\n",
    "                                index = np.arange(1, predicted_labels.shape[0] + 1),\n",
    "                                columns=[target])\n",
    "    predicted_df.to_csv(out_file, index_label=index_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(C=1.0, random_state=17, solver='lbfgs', max_iter=500).fit(X_train_new, y_train)\n",
    "\n",
    "# Make a prediction for test data set\n",
    "y_test = lr.predict_proba(X_test_new)[:, 1]\n",
    "\n",
    "# Write it to the file which could be submitted\n",
    "write_to_submission_file(y_test, 'baseline_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr1 = LogisticRegression(C=0.01, random_state=17, solver='lbfgs', max_iter=2000).fit(X_train_new, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test1 = lr.predict_proba(X_test_new)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_submission_file(y_test1, 'baseline_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9240884454299088"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_auc_lr_valid(X_train_new, y_train, C = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.9537978270268442 - hour\n",
    "# 0.9546807898448859 - hour + year\n",
    "# 0.9547075096976458 - hour + year + morning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
